\section{Conclusions}
\label{section:conclusions}
\vspace{-.35em}

Instruction-tuned LLMs have emerged as a promising means of achieving zero-shot performance with smaller models that is competitive to, and sometimes even better than, that observed using much larger LLMs \cite{longpre2023flan,alpaca}.
In this work we empirically characterized the \emph{robustness} of such models with respect to instruction rephrasings.
In particular, we collected manually composed instructions from 36 graduate students in NLP across 75 tasks, and we evaluated different families of instruction-tuned LLMs (Flan, Alpaca, and T0) when provided observed and unobserved instructions (seen in training and not, respectively). %(i.e., seen in training) and unobserved (manual instructions we collected).
We found that using the latter consistently degrades model performance, indicating that models are unduly sensitive to instruction phrasings.

We then proposed a simple mechanism intended to improve the robustness of instruction-tuned LLMs.
This approach entails introducing an additional loss term that penalizes the model for inducing dissimilar distributions over output tokens when using (a) paraphrased instructions as opposed to (b) a reference instruction for the same task.
We found that training under this objective consistently (though modestly) improves results, and in particular mitigates the degradation observed when previously unobserved instructions are used.

\section{Limitations}
\label{section:limitations}
\vspace{-.35em}

This work has important limitations: For example we only evaluated ``mid-sized'' models (<20B parameters), it is unclear if our findings would generalize to much larger instruction-tuned models. (However, we note that instruction tuning has been most promising for smaller models.)
We also restricted our evaluation to three task types: QA and multi-class and binary classification.

\vspace{0.1em}
\noindent{\bf Ethics} This work does not have an explicit ethical dimension, but we acknowledge that all LLMs are likely to encode problematic biases; it is unclear how instruction-tuning might interact with these.

\section{Acknowledgments}

This work was supported by the National Science Foundation (NSF) grant 1901117.

We thank Jay DeYoung and Alberto Mario Ceballos Arroyo for their advice and feedback on the paper.
We also thank Alberto Mario Ceballos Arroyo, Arnab Sen Sharma, Bowen Zhao, Eric Todd, Hanming Li, Hiba Ahsan, Hye Sun Yun, Shulin Cao, Jay DeYoung, Jered McInerney, Ji Qi, Jifan Yu, Jize Jiang, Kaisheng Zeng, Koyena Pal, Kundan Krishna, Linxiao Nie, Hailong Jin, Jinxin Matthew Liu, Millicent Li, Monica Munnangi, Nikhil Prakash, Pouya Pezeshpour, Sanjana Ramprasad, Sarthak Jain, Shangqing Tu, Somin Wadhwa, Tingjian Zhang, Hao Wesley Peng, Xiaozhi Wang, Xingyu Lu, Xin Lv, Zijun Yao for providing manually written instructions.




\end{table*}
