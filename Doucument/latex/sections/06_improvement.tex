\section{Aligning Equivalent Instructions}
\vspace{-0.5em}
\label{section:methods}

%As an attempt to address the issues discussed above, 
We now introduce a simple, lightweight, but effective method to improve the robustness of instruction-tuned LLMs.
The intuition is to introduce a term in the objective which explicitly encourages the model to yield similar predictions (and hence similar representations) for the same input when provided distinct but semantically equivalent instructions.


%\subsection{Aligning Instructions}

%For an autoregressive Language Model with
More specifically, we aim to align semantically equivalent instructions in the space induced by the model.
To this end we introduce soft embedding parameters with dimensions $\mathbb{R}^{d \times n}$; this is equivalent to adding $n$ novel tokens (with embedding dimension $d$) as prefixes to inputs (preceding instructions).
The intuition is to push the representations for semantically equivalent tasks close together.
To this end, we add additional term to the loss: The KL-divergence $\mathcal{L}_{\text{KL}}$ of the output probabilities between a reference instruction for a given task and paraphrased (semantically equivalent) version of the same.
We combine this with the standard cross-entropy loss, and fine-tune \emph{only} the introduced soft prompt parameters under this objective (Figure \ref{fig:method-schematic}).
Here $\lambda$ is a loss-weighting hyper-parameter, $\hat{y}^{(j)}_i$ and $\hat{y}_r^{(j)}$ are the distributions over the vocabulary $\mathcal{V}$ induced by the model with paraphrased instruction $i$ and the reference instruction $r$ at token position $j$.\footnote{We pad instances such that the lengths in a given batch are effectively equal; the sum is therefore from 1 to the length associated with the current batch, we omit this for simplicity.}

%observed instructions and train the prefix embedding to "drag" the unobserved and observed instruction closer together.
%, we introduce a new trainable parameter $\theta'\in \mathbb{R}^{d\times n}$ as the soft prefix embedding adding at the front of input instruction to "align" the unobserved instruction with the observed instruction. 

%To achieve the goal,
%we collect a set of paraphrases (see 5.2 for more details) of the 

%Hence, we freeze the model and fine-tune the additional parameters $\theta'$.

%We process each batch of training data %into $\{x_i, y_i\}_{|i=1...N+1}$, where $x_1=I(x)$ and $x_{i\neq1}=I'_{i-1}(x)$. 
%In other words,
%such that each batch comprises instances that share semantically equivalent instructions.
%One example (the first, arbitrarily) is always the original sample from the observed instruction tuning set. 
%first data is always the original sample from the observed training set.
%We then jointly optimize the soft embedding parameters with respect to a linear combination of KL-divergence (between output distributions for an instance given distinct but semantically equivalent instructions) and the standard cross-entropy loss for the corresponding target. 




%At each step during training, we optimize instances with paraphrased instructions and original instructions separately with different objectives. 
%For original instruction, we optimize it with the cross-entropy loss $\mathcal{L}_{CE}$ with the ground truth. For instance, with  paraphrased instructions, instead of learning the correct answer, we optimize the KL-divergence $\mathcal{L}_{KL}$ of the output probabilities between it and its observed counterpart. The overall loss of each training batch is given as follows:


\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.6\textwidth}
    \centering
    %{\includegraphics[scale=0.365]{images/method-fig.pdf}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.125\textwidth}
    \begin{align*}
       & \mathcal{L} = (1-\lambda)\mathcal{L}_{\text{CE}} + \lambda\mathcal{L}_{\text{KL}}                              \\
       & \mathcal{L}_{\text{KL}} = \frac{1}{N-1}\sum_{i\neq r}^{N}\sum_j \text{KL}(\hat{y}_{i}^{(j)} ||\hat{y}_r^{(j)}) \\
       & \hat{y}_{i}^{(j)} = \text{Softmax}(p_{i}^{(j)})\text{, }p_{i}^{(j)}\in\mathbb{R}^{|\mathcal{V}|}               \\
    \end{align*}
  \end{subfigure}
  \caption{Schematic depiction of the proposed instruction alignment method (left) and associated loss terms (right). Dotted (red) lines indicate backpropagation; we update only the soft prompt parameters, which we show yields performance superior to fine-tuning all model parameters.
    \label{fig:method-schematic}}
\end{figure}

\begin{comment}
\begin{tabular}{p{4cm}c}
  {
    \begin{align*}
       & \mathcal{L} = (1-\lambda)\mathcal{L}_{\text{CE}}(\hat{y}_{1}, y_1) + \lambda\mathcal{L}_{\text{KL}}      \\
       & \mathcal{L}_{\text{KL}} = \frac{1}{N-1}\sum_{i=2}^{N}\sum \text{KL}(\hat{y}_{i}^{(j)} ||\hat{y}_1^{(j)}) \\
       & \hat{y}_{i}^{(j)} = \text{Softmax}(p_{i}^{(j)})\text{, }p_{i}^{(j)}\in\mathbb{R}^{|v|}                   \\
    \end{align*}
  } & {\includegraphics[scale=0.4]{images/method-fig.pdf}}
\end{tabular}
\end{comment}

%\subsection{Dataset Preparation}

\begin{comment}
\begin{table}%[h]
  \small
  \centering
  \begin{tabular}{c l c c c }
    \toprule
    \textbf{Collection} & \textbf{Task}                    & \textbf{Type} & \textbf{Instances} & \textbf{\# of Paraphrases} \\
    \midrule
    \textbf{NIV2}       & \textsc{CommonsenseQA}           & QA            & 120                & 14                         \\
    \textbf{NIV2}       & \textsc{AI2 ARC-Challenge}       & QA            & 120                & 14                         \\
    \textbf{NIV2}       & \textsc{OpenbookQA}              & QA            & 120                & 14                         \\
    \textbf{NIV2}       & \textsc{MathQA General}          & QA            & 120                & 14                         \\
    \textbf{NIV2}       & \textsc{XCSR MC Classification}  & MC            & 120                & 14                         \\
    \textbf{NIV2}       & \textsc{Newscomm Classification} & MC            & 120                & 14                         \\
    \textbf{NIV2}       & \textsc{TriviaQA Classification} & BC            & 120                & 14                         \\
    \textbf{NIV2}       & \textsc{MultiRC Classification}  & BC            & 120                & 14                         \\
    \hline
    \textbf{Alpaca}     & \textsc{Alpaca}                  & General       & 500                & 9                          \\
    \bottomrule
  \end{tabular}
  \vspace{-0.5em}
  \caption{Data used for soft prompt alignment; we paraphrase reference instructions using GPT-4.}
  \label{tab:data_stat}
\end{table}
\vspace{-0.5em}

\end{comment}

Optimizing for the above objective requires paraphrased instructions $i$ for each task in the training data; we generate these automatically as follows.
For instruction-tuning dataset, we sample a small amount of training data to use for alignment. %to serve as the instances for soft prompt alignment. 
We paraphrase these reference instructions using GPT-4. For the Alpaca collection, we randomly sampled 1000 tasks and paraphrased them with three prompts, and collected the top three candidates under temperature 0.5. For the Flan collection, we randomly sampled 986 instances from the mixture with 3 prompts with greedy decoding. % Table \ref{tab:data_stat} reports counts yielded for different tasks we assembled in this way. 

For fine-tuning, we then create instances for each example by pairing them with every distinct instruction available for the corresponding task.
We then form batches by including one instance featuring the original instruction and the rest comprising paraphrased instructions. For the implementation of the prefix, we follow the setting of \cite{li2021prefix}, which freezes the model parameters and just trains the prefix embeddings with the MLP layers.
%For each instruction (1 origin instruction + paraphrases) of the dataset, we combine it with all the instances. 
%Hence, for each instance with the original instruction $I$, we have N counterparts with paraphrased instruction $I'_{n}$ for $n\in \{1...N\}$.  


\section{Results}

We experiment with the proposed method using two representative instruction-tuned LLMs: Flan-XL (3B) and Alpaca (7B).
We compare the canonical versions of these models trained in the usual way (the same evaluated in Table \ref{tab:main_result}) to variants fine-tuned using our proposed approach.
We ablate components of our method to tease out the contributions of data and objectives.
%components of the proposed alignment strategy. 
%That is, we ablate the components of the proposed approach and report corresponding results to tease out the contributions of data and objectives. 

Specifically, we consider variants where we: Fine-tune all model parameters on the additional, automatically generated instruction paraphrases (FT); impose the new KL loss term (again fine-tuning all model parameters; FT+KL); introduce the additional soft prompt parameters and fine-tune on the paraphrase instances, but without KL (PT); and then the full proposed strategy, which introduces the soft prompt parameters and optimizes them for the loss augmented with the KL term ({\bf PT+KL}).

%We compare variants of these models fine-tuned on the corresponding instruction datasets in the usual way to results achieved 
%For fine-tuning the baseline, we compare the performance of fine-tuning and so. More implementation details can be seen in Appendix ?

\begin{table}[h]
  \centering
  \small
  \begin{tabular}{l c c c c c c }
    \toprule
                        & \multicolumn{3}{c}{\textsc{MMLU}}                      & \multicolumn{3}{c}{\textsc{BBL}}                                                                                                                                                                                                                                                           \\ [0.5ex]
    \midrule
    \textbf{Model}      & \textsc{Obs.}                                          & \textsc{Unobs.}                                        & Avg.                                                   & \textsc{Obs.}                                          & \textsc{Unobs.}                                        & Avg.                                                   \\
    \textsc{Flan-T5-3B} & 48.1                                                   & 47.5                                                   & 47.8                                                   & \textbf{56.1}                                          & 51.9                                                   & 54.0                                                   \\
    FT                  & 39.4 \textcolor{red}{\textbf{(-8.7)}}                  & 40.1 \textcolor{red}{\textbf{(-7.4)}}                  & 39.8 \textcolor{red}{\textbf{(-8.0)}}                  & 48.2 \textcolor{red}{\textbf{(-7.9)}}                  & 42.3 \textcolor{red}{\textbf{(-9.2)}}                  & 45.3 \textcolor{red}{\textbf{(-8.7)}}                  \\
    FT+KL               & 41.8 \textcolor{red}{\textbf{(-6.3)}}                  & 43.6 \textcolor{red}{\textbf{(-3.9)}}                  & 45.9 \textcolor{red}{\textbf{(-1.9)}}                  & 47.7 \textcolor{red}{\textbf{(-8.4)}}                  & 43.1 \textcolor{red}{\textbf{(-8.8)}}                  & 45.4 \textcolor{red}{\textbf{(-8.6)}}                  \\
    PT                  & 48.1 \textcolor{MidnightBlue}{\textbf{(+0.0)}}         & 47.6 \textcolor{ForestGreen}{\textbf{(+0.1)}}          & 47.9 \textcolor{ForestGreen}{\textbf{(+0.1)}}          & 55.9 \textcolor{red}{\textbf{(-0.2)}}                  & 52.1 \textcolor{ForestGreen}{\textbf{(+0.2)}}          & 54.0 \textcolor{MidnightBlue}{\textbf{(+0.0)}}         \\
    \textbf{PT+KL}      & \textbf{48.1} \textcolor{ForestGreen}{\textbf{(+0.1)}} & \textbf{47.9} \textcolor{ForestGreen}{\textbf{(+0.4)}} & \textbf{48.0} \textcolor{ForestGreen}{\textbf{(+0.2)}} & 55.9 \textcolor{red}{\textbf{(-0.2)}}                  & \textbf{53.7} \textcolor{ForestGreen}{\textbf{(+1.8)}} & \textbf{54.8} \textcolor{ForestGreen}{\textbf{(+0.8)}} \\
    \midrule
    \textsc{Alpaca-7B}  & 41.9                                                   & 39.7                                                   & 40.8                                                   & 47.6                                                   & 42.9                                                   & 45.3                                                   \\
    FT                  & 40.3 \textcolor{red}{\textbf{(-1.6)}}                  & 39.1 \textcolor{red}{\textbf{(-0.6)}}                  & 39.7 \textcolor{red}{\textbf{(-1.1)}}                  & 44.4 \textcolor{red}{\textbf{(-3.2)}}                  & 42.1 \textcolor{red}{\textbf{(-0.8)}}                  & 43.4 \textcolor{red}{\textbf{(-2.0)}}                  \\
    FT+KL               & 39.7 \textcolor{red}{\textbf{(-2.2)}}                  & 40.2 \textcolor{ForestGreen}{\textbf{(+0.5)}}          & 40.0 \textcolor{red}{\textbf{(-0.8)}}                  & 45.6 \textcolor{red}{\textbf{(-2.0)}}                  & 42.8 \textcolor{red}{\textbf{(-0.1)}}                  & 44.2 \textcolor{red}{\textbf{(-1.1)}}                  \\
    PT                  & 42.1 \textcolor{ForestGreen}{\textbf{(+0.2)}}          & 40.0 \textcolor{ForestGreen}{\textbf{(+0.3)}}          & 41.1 \textcolor{ForestGreen}{\textbf{(+0.3)}}          & 47.5 \textcolor{red}{\textbf{(-0.1)}}                  & 43.0 \textcolor{ForestGreen}{\textbf{(+0.1)}}          & 45.3 \textcolor{MidnightBlue}{\textbf{(+0.0)}}         \\
    \textbf{PT+KL}      & \textbf{42.4} \textcolor{ForestGreen}{\textbf{(+0.5)}} & \textbf{41.8} \textcolor{ForestGreen}{\textbf{(+2.1)}} & \textbf{42.1} \textcolor{ForestGreen}{\textbf{(+1.3)}} & \textbf{47.9} \textcolor{ForestGreen}{\textbf{(+0.3)}} & \textbf{46.6} \textcolor{ForestGreen}{\textbf{(+3.7)}} & \textbf{47.3} \textcolor{ForestGreen}{\textbf{(+2.0)}} \\
    \bottomrule
  \end{tabular}
  \caption{Results and ablations of the proposed soft prompt alignment method. All ablated versions use the augmented set with automatically paraphrased instructions. FT refers to simply fine-tuning (with teacher-forcing) on this additional data; PT denotes prefix tuning (i.e., introducing soft prompt parameters); KL refers to the alignment objective that we proposed above. Using all of these components together yields the best performance, especially on unobserved instructions.}
  \label{tab:alignment-results}
\end{table}

We report results in Table \ref{tab:alignment-results}.
Two observations: (1) The proposed soft prompt alignment strategy ({\bf PT+KL}) yields consistent improvements across the tasks and models considered and especially improves performance on unobserved instructions, as anticipated. (2) The full benefit of the approach is realized only when all components---the additional automatically paraphrased training instructions, soft prompt parameters, and additional KL loss term---are in place.

\begin{table*}[h]
  \small
  \centering
  \begin{tabular}{l l l l}
    \toprule
    \textbf{Dataset} & \textbf{Closest Distance Before} & \textbf{Closest Distance After} & \textbf{$\Delta$ Acc. Improvement} (\%) \\
    \midrule
    \textsc{MMLU}    & 22.2                             & 21.3                            & + 0.3\%                                 \\
    \midrule
    \textsc{BBL QA}  & 22.4                             & 23.0                            & + 0.4\%                                 \\
    \textsc{BBL BC}  & 30.1                             & \textbf{27.9}                   & \textbf{+ 4.2\%}                        \\
    \textsc{BBL MC}  & 26.0                             & 24.6                            & + 0.3\%                                 \\
    \bottomrule
  \end{tabular}
  \caption{Average distances before and after soft prompt alignment with Flan-T5-XL.} %Ten observed and unobserved instructions with 50 instances for each dataset.} %For each unobserved instruction, its closest observed instruction is found by the average $\ell$2 distances across instances. We report the result of all the datasets in the Appendix}
  \vspace{-0.5em}
  \label{tab:improvement_distance}
\end{table*}


Following our approach in \ref{section:mmlu_variance}, we take the average distance between observed and unobserved instructions before and after alignment.
Table \ref{tab:improvement_distance} shows that our method brings observed and unobserved instruction representations closer together.
The similarity is most increased in the case of the biggest accuracy gain, further suggesting the mechanism of improvement provided by soft prompt alignment.
%Further, a large reduction of distance is observed in most of the binary classification datasets, which is also where the improvement is the most significant.


%\subsection{Discussion}
