\section{Instruction Datasets}

\subsection{Evaluation Benchmarks}

%We perform the evaluation of instruction-tuned models 
We evaluate a set of instruction-tuned models on two large benchmarks: \textsc{MMLU} \cite{hendrycks2020measuring} and \textsc{Big-Bench} \cite{srivastava2022beyond}. \textsc{MMLU} is a multiple-choice question-answering benchmark comprising 57 tasks that require expert knowledge.
\textsc{Big-Bench} is a collaboratively built benchmark containing 204 diverse tasks from various domains; here %we use the 18 task 
consider the \textsc{Big-Bench Lite} subset, and we include only QA, multi-class, and binary classification tasks, yielding 18 tasks from in all. %in \textsc{Big-Bench}.

%More specifically, We conduct our experiment over all 57 tasks on MMLU. An 18 tasks subset of \textsc{Big-Bench Lite} consists of all QA, multi-class, and binary classification tasks.

\subsection{Collecting New Instructions from NLP Researchers}
\label{section:new-instructions}

We aim to evaluate instruction-tuned models when they are provided instructions which are semantically equivalent to, but superficially different from, those with which they were trained.
To this end, we enlist NLP researchers (graduate students) to compose novel instructions for the tasks considered; these particular instruction phrasings were therefore \emph{unobserved} during instruction fine-tuning. 

%For each instruction-tuned language model we  we collect a set of instructions unobserved to the model by expert annotation, and we collect a set of instructions observed during training from the original instruction-tuning collection.

% TODO: to have some small tables here to show the stats

%\subsubsection{Unobserved Instruction}

%We perform large-scale crowd-sourcing from 
More specifically, we recruited 36 NLP graduate students working in NLP.
All had at least some experience with instruction-tuned models and the downstream tasks included in the evaluation benchmarks. 
For each of the 18 tasks in \textsc{BBL} and all tasks in \textsc{MMLU}, we asked 12 graduate students to write one (distinct) instruction they would use for zero-shot inference with an instruction-tuned model. 
%To ensure fairness, the information on models is omitted to avoid having priors to fit the pattern of the specific model. 
%The detailed instruction collection process can be seen in Appendix A.
We provide details on this instruction collection process in Appendix A. 
We will release all 319 instructions acquired for this work to ensure the reproducibility of this work and to facilitate further research on instruction-tuned model robustness. 
% We treat 57 tasks of MMLU as a whole (general QA template). Don't know how to make the word clearer

%\subsubsection{Observed Instruction}


% could be generally applied to one of ``multiple-choice QA'', ``binary label classification'', and ``m
%``multi-label classification" tasks. 

% TODO: to have some small tables here to show the stats





