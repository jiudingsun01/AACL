
Instruction-tuned LLMs对于 unseen Instructions 以及 unseen tasks的鲁棒性。
With four real-world relation extraction dataset as case studies, we comprehensively evaluate the two series of instruction-following large language models tuned on 1) open-domain instructions and 2) task-oriented instructions.

Our findings are: 



1) robustness

2) performance

3) size -> 


对FLAN-T5 3b, 11b，Alpaca 7b, Vicuna 7b, WizardLM 7b模型在TACRED，RETACRED，TACREV，SemEval数据集（分为unseen instruction和unseen task）上测试，P,Recall,F1
 
Alpaca，Vicuna，WizardLM 以及traditional task-oridented 

Instruction Dataset包含模型直接学习的指令
Unobserved Instructions是模型训练过程中未直接学习但可能遇到的指令

Unseen Task（分QA和RE）
Unseen Instructions则是模型在训练过程中完全未遇到，需要模型具有高度泛化能力才能处理的指令。
Zero-shot 模型需要在没有看过任何特定任务样本的情况下，直接对任务作出预测

框架

1 Introduction
    A. 介绍研究的背景和目的
    B. 说明所使用的不同模型：FLAN-T5 3b, 11b，Alpaca 7b, Vicuna 7b, WizardLM 7b
    C. 概述研究主题：unseen instruction和unseen task上的鲁棒性
    主要贡献如下： We perform a comprehensive and indepth analysis of the robustness of xxx模型

2 Related Work


3 Instruction Datasets
TACRED，RETACRED，TACREV，SemEval数据集

只有REtask，reformulate 成QA


Unseen Task （分QA和RE）,

QA和RE，再测试instruction的robustness

3.1 Unseen Datasets
	介绍unseen instruction和unseen task的数据集？
3.2 Evaluation Benchmarks
	我们在两个大型测试基准上评估了一组指令调优模型：

4 Evaluating the Robustness of Instruction-tuned LLMs
4.1 Models and Data
我们针对xx指令集合进行实验，这x个集合分别训练了... ...（简单介绍各指令集合，主要是seen/oberved的instructions）

To evaluate model robustness with respect to instruction phrasings we use two benchmarks:... ...（介绍benchmarks）

Table 1 Examples of observed instructions we collected for three general types of tasks.（只做QA还是？）

We use the same instructions for all tasks in the same category, taken from the published instruction
tuning datasets associated with each model. These instructions are general, e.g., in the case of
classification they request that the model consider an example with respect to categorization criteria
and label space provided by the instance, and select an appropriate category (examples in Table 1).
One can “mix-and-match” such instructions so long as they are appropriate for the task type.

4.2 Result（基本照抄）
	We present the main aggregated analysis results in Figure 2 and Table 3. The take-away here is
that using instructions unobserved in training—but manually composed for the task at hand and so
semantically appropriate—leads to considerable degradation in performance: On average, unobserved
instructions reduce accuracy by over five points across models considered. Table 3 reports results
disaggregated by task type; we observe that classification tasks are most harmed by use of novel
instructions. We provide additional, more granular (dataset-level) results in the Appendix.

4.3 A Closer Look at Instruction Robustness
	Above we used general instructions requesting the model to perform tasks (Table 1). Here we delve
further into the performance degradation observed when using novel instructions. We report a curious
result highlighting the degree to which models rely on having previously observed instructions:
Incorrect but observed instructions outperform appropriate but unobserved instructions (Figure 3).做一下Incorrect但observed的instruction表现和appropriate but unobserved表现对比，出一个对比的大表**

挑其中一两个结果做分析

4.4 Scaling
	Does instruction robustness begin to emerge as a function of scale?To attempt to answer this, we
repeated all experiments from Table 3 with Flan-T5 model sizes ranging from small (80M parameters)
to XXL (11B).We observe in Figure 2b that the disparity between results achieved with observed
versus unobserved instructions does not seem to decrease with model scale, at least up to this point.
That said, massive models (175B+) may offer greater robustness. However, we reiterate that much of
the excitement about instruction tuning is the possibility that this technique appears to allow much
smaller models to achieve results competitive with massive alternatives.
同样的实验重跑一遍测下规模的影响，出对比图，大概率结论和他提供的相同，不会随着规模增大而减小

4.5 Robustness with Semantic Distance不知道做不做

4.6到后面的内容不做，不搞对齐

5 Conclusions

6 Limitations

参考文献



00 abstract
  \emph{Instruction fine-tuning} has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks.  
This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants.
In this paper we 主要讨论  How can we make them more robust to such natural language variation? 
To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. 
We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. 
Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. 
We propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models. \footnote{The code and instructions are publicly available at: \url{https://github.com/jiudingsun01/InstructionEval}}

Content:
The focus is on the robustness of instruction-tuned Large Language Models (LLMs) to unseen instructions and unseen tasks.  We conducted an exploration on four models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models using real-world relation extraction datasets as case studies.  We carried out a comprehensive evaluation of these instruction-following large language models which have been tuned based on open-domain instructions and task-oriented instructions.  The central discussion is on how to enhance their robustness to natural language variation.  We observed that xxxx  This consistently improves the robustness of the instruction-tuned models.

Instruction-tuned LLMs对于unseen Instructions 和tasks的鲁棒性影响。通过四个real-world relation
extraciton数据集为case studies，我们全面评估了两种经过调校的大型语言模型系列：一种是针对"开放领域(open-domain)指令"调校的，另一种是针对"特定任务(task-oriented)指令"调校的。


01 Introduction

In this work
we investigate how robust instruction-tuned models are. More specifically, we ask: How sensitive are
instruction-tuned LMs to shifts in instruction phrasings at test time? This is particularly important
given that the primary motivation of instruction tuning is to facilitate zero-shot adaptation via natural
language instruction: If models are overly sensitive to the particular phrasing of a task instruction it
may greatly limit their utility in practice