
Instruction-tuned LLMs对于 unseen Instructions 以及 unseen tasks的鲁棒性。
With four real-world relation extraction dataset as case studies, we comprehensively evaluate the two series of instruction-following large language models tuned on 1) open-domain instructions and 2) task-oriented instructions.

Our findings are: 



1) robustness

2) performance

3) size -> 
Flan-T5-3B Alpaca-7B Vicuna-7B T0++ 11B Flan-T5-11B WizardLM-13B 

对FLAN-T5 3b, 11b，Alpaca 7b, Vicuna 7b, WizardLM 13b模型在TACRED，RETACRED，TACREV，SemEval数据集（分为unseen instruction和unseen task）上测试，P,Recall,F1
 
Alpaca，Vicuna，WizardLM 以及traditional task-oridented 

Instruction Dataset包含模型直接学习的指令
Unobserved Instructions是模型训练过程中未直接学习但可能遇到的指令

Unseen Task（分QA和RE）
QA Seen
Unseen Instructions则是模型在训练过程中完全未遇到，需要模型具有高度泛化能力才能处理的指令。
Zero-shot 模型需要在没有看过任何特定任务样本的情况下，直接对任务作出预测

1. 探索模型对于Unseen tasks(RE)和Seen tasks(QA)的应对paraphrase的鲁棒性，把datasets里的QA和RE拿出来分别修改instructions(paraphrase)在所有模型上跑一遍，对比性能变化
2. 分别做zero-shot和few-shot，观察in context learning是否能改善模型鲁棒性
3. 用Flan-T5验证模型的scaling(small～XXL)能否改善模型鲁棒性


框架

评估LLM对Instruction的鲁棒性

处理后的数据集包括Unseen task和seen task，Unseen task下为RE Instructions，seen task下为QA Instructions。
我们的目标是评估LLMs在对于unseen和seen task下的RE和QA Instructions 的鲁棒性，促进Fine-tuned LLMs鲁棒性相关工作的进一步研究。

1 Introduction
    A. 介绍研究的背景和目的
    B. 说明所使用的不同模型：Flan-T5-3B Alpaca-7B Vicuna-7B T0++ 11B Flan-T5-11B WizardLM-13B 
    C. 概述研究主题：unseen instruction和unseen Instruction上的鲁棒性
    主要贡献如下： We perform a comprehensive and indepth analysis of the robustness of xxx模型

2 Related Work

3 Evaluating the Robustness
3.1 DataSets and Models
我们选取了TACRED，RETACRED，TACREV，SemEval这四个RE数据集，每个RE数据集都已经reformulate对应QA（cite Kai Zhang），分别包含一个Instructions。为了更好的对Instructions的鲁棒性进行评估，我们对于QA和RE数据集上的Instruction人工进行重写，在保证diverse的基础上分别新增了三个Instruction。

Table 1 

经过处理后的TACRED，RETACRED，TACREV，SemEval数据集分别包含4个QA和RE的Instructions，对应seen和unseen两个task。在数据集中，我们对同一task的所有任务使用相同的Instructions，这些指令是通用的，例如在QA的情况下，要求模型在给定sentence和choice的情况下完成对选项的判断Which option can be inferred from the given Sentence（参见表格1的例子）。

完成数据集准备后，我们分别对Flan-T5-3B Alpaca-7B Vicuna-7B T0++ 11B Flan-T5-11B WizardLM-13B这六个Fine-tuned LLMs全面评估了QA、RE Instruction的鲁棒性。

3.2 Result（基本照抄）

图2 在测试中使用unseen task下的RE Instructions会降低Fine-tuned LLMs的性能（a）, 规模大小不一定能解决这个问题(b)
(a)使用seen和unseen task下的Instructions时，模型的平均zero-shot性能(b)使用seen和unseen task下的Instructions时，不同模型大小的Flan-T5性能表现

表3 在6个模型上使用seen task(QA instructions)和unseen task(RE instructions)的结果（按类型分组）。当使用RE Instructions时，性能会降低——sometimes by xxxpoint这表明经过fine-tuned的模型的鲁棒性不强。

我们在图2和表3中呈现了测试结果，其中表3包含了6个模型在数据集上不同task下的性能和表现。从图2中可以发现在使用unseen task下的RE Instructions时性能相较于seen task下的QA Instructions都低。表3的结果是按照task类型进行划分的，我们发现，使用unseen task下的Instructions对LLMs的鲁棒性影响较大.

	We present the main aggregated analysis results in Figure 2 and Table 3. The take-away here is
that using instructions unobserved in training—but manually composed for the task at hand and so
semantically appropriate—leads to considerable degradation in performance: On average, unobserved
instructions reduce accuracy by over five points across models considered. Table 3 reports results
disaggregated by task type; we observe that classification tasks are most harmed by use of novel
instructions. We provide additional, more granular (dataset-level) results in the Appendix.


3.3 A Closer Look at Instruction Robustness 再说
	Above we used general instructions requesting the model to perform tasks (Table 1). Here we delve
further into the performance degradation observed when using novel instructions. We report a curious
result highlighting the degree to which models rely on having previously observed instructions:
Incorrect but observed instructions outperform appropriate but unobserved instructions 





挑其中一两个结果做分析

3.4 Scaling
	Does instruction robustness begin to emerge as a function of scale?To attempt to answer this, we
repeated all experiments from Table 3 with Flan-T5 model sizes ranging from small (80M parameters)
to XXL (11B).We observe in Figure 2b that the disparity between results achieved with observed
versus unobserved instructions does not seem to decrease with model scale, at least up to this point.
That said, massive models (175B+) may offer greater robustness. However, we reiterate that much of
the excitement about instruction tuning is the possibility that this technique appears to allow much
smaller models to achieve results competitive with massive alternatives.
同样的实验重跑一遍测下规模的影响，出对比图，大概率结论和他提供的相同，不会随着规模增大而减小

4 Shot
做一下Zero-shot, One-shot, Few shot，

5 Conclusions

6 Limitations

参考文献



00 abstract
  \emph{Instruction fine-tuning} has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks.  
This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants.
In this paper we 主要讨论  How can we make them more robust to such natural language variation? 
To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. 
We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. 
Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. 
We propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models. \footnote{The code and instructions are publicly available at: \url{https://github.com/jiudingsun01/InstructionEval}}

Content:
The focus is on the robustness of instruction-tuned Large Language Models (LLMs) to unseen instructions and unseen tasks.  We conducted an exploration on four models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models using real-world relation extraction datasets as case studies.  We carried out a comprehensive evaluation of these instruction-following large language models which have been tuned based on open-domain instructions and task-oriented instructions.  The central discussion is on how to enhance their robustness to natural language variation.  We observed that xxxx  This consistently improves the robustness of the instruction-tuned models.

Instruction-tuned LLMs对于unseen Instructions 和tasks的鲁棒性影响。通过四个real-world relation
extraciton数据集为case studies，我们全面评估了两种经过调校的大型语言模型系列：一种是针对"开放领域(open-domain)指令"调校的，另一种是针对"特定任务(task-oriented)指令"调校的。


01 Introduction

In this work
we investigate how robust instruction-tuned models are. More specifically, we ask: How sensitive are
instruction-tuned LMs to shifts in instruction phrasings at test time? This is particularly important
given that the primary motivation of instruction tuning is to facilitate zero-shot adaptation via natural
language instruction: If models are overly sensitive to the particular phrasing of a task instruction it
may greatly limit their utility in practice